{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NLP-101","text":"<p>Welcome to NLP-101 - a comprehensive introduction to Natural Language Processing (NLP), covering fundamental concepts and advanced techniques that power modern NLP systems.</p>"},{"location":"#what-is-nlp","title":"What is NLP?","text":"<p>Natural Language Processing is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to read, understand, and derive meaning from human languages in a valuable way.</p>"},{"location":"#course-structure","title":"Course Structure","text":"<p>This course is organized into modules, each covering essential aspects of NLP:</p>"},{"location":"#1-language-modeling","title":"1. Language Modeling","text":"<p>Learn about the fundamentals of language modeling, including tokenization techniques and how to build language models from scratch.</p>"},{"location":"#about-this-course","title":"About This Course","text":"<p>NLP-101 provides: - Clear, well-documented implementations of core NLP concepts - Detailed explanations of algorithms and techniques - Hands-on examples you can experiment with - Progressive learning from basics to advanced topics</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Explore the course modules in the navigation menu, or clone the repository to experiment with the code:</p> <pre><code>git clone https://github.com/saqlain2204/Language-Modeling-101.git\ncd Language-Modeling-101\n</code></pre> <p>Created by Saqlain</p>"},{"location":"Language-Modeling/","title":"Language Modeling","text":"<p>Welcome to the Language Modeling module of NLP-101. This section covers fundamental concepts and implementations in language modeling.</p>"},{"location":"Language-Modeling/#topics-covered","title":"Topics Covered","text":"Topic Subtopic Description Resources Tokenization Byte Pair Encoding (BPE) Subword tokenization algorithm that learns to represent common character sequences as single tokens BPE Guide, Python Implementation"},{"location":"Language-Modeling/#what-is-language-modeling","title":"What is Language Modeling?","text":"<p>Language modeling is a fundamental task in Natural Language Processing that involves predicting the probability of a sequence of words. It forms the backbone of many modern NLP applications, from machine translation to text generation.</p>"},{"location":"Language-Modeling/#learning-objectives","title":"Learning Objectives","text":"Objective Description Basic Concepts Understand fundamental concepts of language modeling and probability distributions over text Tokenization Techniques Learn different tokenization approaches including character, word, and subword tokenization Implementation Implement and experiment with various language modeling approaches from scratch"},{"location":"Language-Modeling/Tokenization/BPE/","title":"Byte Pair Encoding (BPE)","text":"<p>Basic Idea: Train the tokenizer on raw text to automatically determine the vocabulary.</p> <p>Intuition: Common sequences of characters are represented by a single token, rare sequences are represented by many tokens.</p> <p>Algorithm Sketch: 1. Start with each character as a token (plus an end-of-word marker). 2. Count all adjacent token pairs in the corpus. 3. Merge the most frequent pair into a new token. 4. Repeat for a fixed number of merges.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#how-it-works","title":"How It Works","text":"<p>BPE is an algorithm originally used for data compression. In NLP, it's used to build subword vocabularies:</p> <ol> <li>Initialization: Start with all characters in your corpus as separate tokens</li> <li>Iterative Merging: Repeatedly merge the most frequent adjacent pair of tokens</li> <li>Stopping Criterion: Stop after a certain number of merges (vocabulary size)</li> </ol>"},{"location":"Language-Modeling/Tokenization/BPE/#step-by-step-bpe-process","title":"Step-by-Step BPE Process","text":"<p>Let's walk through a simplified example of how BPE works with these words: <pre><code>low lower lowest\nnewer wider\n</code></pre></p>"},{"location":"Language-Modeling/Tokenization/BPE/#initial-vocabulary","title":"Initial Vocabulary","text":"<p>Each word is split into characters with an end-of-word marker: - <code>l, o, w, &lt;/w&gt;</code> (low) - <code>l, o, w, e, r, &lt;/w&gt;</code> (lower) - <code>l, o, w, e, s, t, &lt;/w&gt;</code> (lowest) - <code>n, e, w, e, r, &lt;/w&gt;</code> (newer) - <code>w, i, d, e, r, &lt;/w&gt;</code> (wider)</p>"},{"location":"Language-Modeling/Tokenization/BPE/#first-few-merge-operations","title":"First Few Merge Operations","text":"<ol> <li>Find most frequent pair: <code>e, r</code> (appears 3 times)</li> <li>Merge: <code>e + r \u2192 er</code></li> <li> <p>Vocabulary becomes: <code>l, o, w, &lt;/w&gt;</code>, <code>l, o, w, er, &lt;/w&gt;</code>, <code>l, o, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>l, o</code> (appears 3 times)</p> </li> <li>Merge: <code>l + o \u2192 lo</code></li> <li> <p>Vocabulary becomes: <code>lo, w, &lt;/w&gt;</code>, <code>lo, w, er, &lt;/w&gt;</code>, <code>lo, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>lo, w</code> (appears 3 times)</p> </li> <li>Merge: <code>lo + w \u2192 low</code></li> <li>Vocabulary becomes: <code>low, &lt;/w&gt;</code>, <code>low, er, &lt;/w&gt;</code>, <code>low, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></li> </ol> <p>...and so on.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#visual-progression-for-lowest","title":"Visual Progression for \"lowest\"","text":"<pre><code>Initial:  l   o   w   e   s   t   &lt;/w&gt;\nMerge 1:  l   o   w   e   s   t   &lt;/w&gt;  (er doesn't appear)\nMerge 2:  lo  w   e   s   t   &lt;/w&gt;      (lo created)\nMerge 3:  low e   s   t   &lt;/w&gt;          (low created)\n...\nFinal:    low est &lt;/w&gt;                   (after additional merges)\n</code></pre> <p>This demonstrates how the algorithm gradually builds larger subword units based on frequency.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#implementation","title":"Implementation","text":"<p>The Python implementation includes several key functions that work together to perform Byte Pair Encoding:</p>"},{"location":"Language-Modeling/Tokenization/BPE/#1-statistics-collection-function","title":"1. Statistics Collection Function","text":"<pre><code>def get_stats(vocab):\n    pairs = Counter()\n    for word, freq in vocab.items():\n        symbols = word\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n</code></pre> <p>This function counts the frequency of adjacent token pairs in the vocabulary. For each word in our vocabulary: - It iterates through all adjacent pairs of symbols - It increments a counter for each pair, weighted by how frequently the word appears - The result is a Counter object mapping each pair to its frequency across the corpus</p>"},{"location":"Language-Modeling/Tokenization/BPE/#2-vocabulary-merging-function","title":"2. Vocabulary Merging Function","text":"<pre><code>def merge_vocab(pair, vocab):\n    new_vocab = {}\n    bigram = ''.join(pair)\n    for word in vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_vocab[tuple(new_word)] = vocab[word]\n    return new_vocab\n</code></pre> <p>This function applies a specific merge operation to the entire vocabulary: - It creates a new bigram by joining the pair of tokens - For each word in the vocabulary, it replaces all occurrences of the specific pair with the new merged token - It preserves word frequencies in the new vocabulary - It returns a new vocabulary with the merge applied throughout</p>"},{"location":"Language-Modeling/Tokenization/BPE/#3-bpe-training-function","title":"3. BPE Training Function","text":"<pre><code>def bpe(corpus, num_merges):\n    vocab = Counter()\n    for line in corpus:\n        for word in line.split():\n            vocab[tuple(word) + ('&lt;/w&gt;',)] += 1\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = merge_vocab(best, vocab)\n        merges.append(best)\n    return merges, vocab\n</code></pre> <p>The main BPE algorithm function that: - Initializes a vocabulary where each word is split into individual characters plus an end-of-word marker <code>&lt;/w&gt;</code> - Repeatedly:   - Counts pair frequencies with <code>get_stats()</code>   - Finds the most frequent pair   - Applies the merge operation with <code>merge_vocab()</code>   - Records the merge for later use in encoding - Returns both the sequence of merges and the final vocabulary</p>"},{"location":"Language-Modeling/Tokenization/BPE/#4-encoding-function","title":"4. Encoding Function","text":"<pre><code>def encode(word, merges):\n    word = tuple(word) + ('&lt;/w&gt;',)\n    for pair in merges:\n        i = 0\n        new_word = []\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(''.join(pair))\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        word = tuple(new_word)\n    # Handle the end-of-word marker\n    if word[-1] == '&lt;/w&gt;':\n        return word[:-1]\n    else:\n        last = word[-1].replace('&lt;/w&gt;', '')\n        return word[:-1] + (last,) if last else word[:-1]\n</code></pre> <p>This function tokenizes new words using the learned BPE merges: - It starts with the word split into individual characters plus the end marker - It applies each merge operation in the same order they were learned during training - It handles the end-of-word marker properly when returning the final tokenized word</p>"},{"location":"Language-Modeling/Tokenization/BPE/#usage-example","title":"Usage Example","text":"<p>Here's a complete example showing how to use the BPE implementation:</p> <pre><code>from bpe import bpe, encode\n\n# Define a small corpus for training\ncorpus = [\n    \"low lower lowest\",\n    \"newer wider\",\n]\n\n# Train the BPE model with 20 merge operations\nmerges, vocab = bpe(corpus, num_merges=20)\n\n# Print the sequence of merges learned\nprint(\"Learned merges:\")\nfor i, merge in enumerate(merges):\n    print(f\"Merge {i+1}: {merge[0]} + {merge[1]} \u2192 {''.join(merge)}\")\n\n# Print the final vocabulary\nprint(\"\\nFinal vocabulary:\")\nfor word, freq in vocab.items():\n    print(f\"{word}: {freq}\")\n\n# Encode a new word using the learned merges\nencoded = encode(\"lowest\", merges)\nprint(\"\\nEncoded 'lowest':\", encoded)\n</code></pre> <p>This example demonstrates: 1. Training: Creating a BPE model from a small corpus 2. Inspection: Viewing the learned merges and resulting vocabulary 3. Application: Using the model to encode a new word</p> <p>When you run this code, you'll see how the word \"lowest\" gets tokenized according to the subword units learned during training.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#advantages-of-bpe","title":"Advantages of BPE","text":"<ul> <li>Handles out-of-vocabulary words gracefully</li> <li>Balances word-level and character-level representations</li> <li>Efficient for morphologically rich languages</li> <li>Widely used in modern NLP systems (GPT, BERT, etc.)</li> </ul> <p>Created by Saqlain.</p>"}]}