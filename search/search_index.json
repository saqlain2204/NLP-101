{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NLP-101 Documentation","text":"<ul> <li>Natural Language Processing concepts and techniques</li> <li>Key modules:<ul> <li>Language Modeling</li> </ul> </li> </ul>"},{"location":"Language-Modeling/","title":"Language Modeling","text":"<p>Welcome to the Language Modeling module of NLP-101. This section covers the core concepts, techniques, and practical implementations in language modeling.</p>"},{"location":"Language-Modeling/#module-topics-resources","title":"Module Topics &amp; Resources","text":"Topic Description Resources Byte Pair Encoding (BPE) Subword tokenization algorithm for representing common character sequences BPE Guide (implementation included in this README) Resource Accounting Tensor memory, precision types, and training efficiency in deep learning Resource Accounting Guide"},{"location":"Language-Modeling/#what-is-language-modeling","title":"What is Language Modeling?","text":"<p>Language modeling is a fundamental task in Natural Language Processing (NLP) that involves predicting the probability of a sequence of words. It is the backbone of many modern NLP applications, including machine translation and text generation.</p>"},{"location":"Language-Modeling/#learning-objectives","title":"Learning Objectives","text":"Objective Description Basic Concepts Understand fundamental concepts of language modeling and probability distributions over text Tokenization Techniques Learn different tokenization approaches including character, word, and subword tokenization Implementation Implement and experiment with various language modeling techniques"},{"location":"Language-Modeling/Resource%20Accounting/","title":"Resource Accounting","text":""},{"location":"Language-Modeling/Resource%20Accounting/#key-points","title":"Key Points","text":"<ul> <li>Tensors are the core data structure in machine learning, used for:</li> <li>Parameters</li> <li>Gradients</li> <li>Optimizer states</li> <li>Data activations</li> </ul>"},{"location":"Language-Modeling/Resource%20Accounting/#tensor-memory-types","title":"Tensor Memory Types","text":"<ul> <li>float32 (single precision):</li> <li>Standard for full precision in ML</li> <li> <p>Good balance of range and accuracy</p> </li> <li> <p>float16 (half precision):</p> </li> <li>Uses less memory</li> <li> <p>Not ideal for very small numbers</p> </li> <li> <p>bfloat16 (brain floating point):</p> </li> <li>Same memory as float32</li> <li> <p>More bits for exponent, fewer for fraction</p> </li> <li> <p>fp8 (8-bit floating point):</p> </li> <li>Very low memory usage</li> </ul> <p>Memory usage depends on: - Number of values - Data type of each value</p>"},{"location":"Language-Modeling/Resource%20Accounting/#training-implications","title":"Training Implications","text":"<ul> <li>Training with float32 is stable but memory-intensive</li> <li>Training with float16, bfloat16, or fp8 saves memory but can cause instability</li> <li>Mixed precision training is common (e.g., float32 for attention, bfloat16 for feed-forward)</li> <li>Tensors are on CPU by default; use GPU for acceleration</li> </ul>"},{"location":"Language-Modeling/Resource%20Accounting/#pytorch-example-device-memory","title":"PyTorch Example: Device &amp; Memory","text":"<pre><code>import torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")\nnum_gpus = torch.cuda.device_count()\nfor i in range(num_gpus):\n    properties = torch.cuda.get_device_properties(i)\n    print(properties)\nmemory_allocation = torch.cuda.memory_allocated() if device == 'cuda' else None\nprint(f\"Memory allocated: {memory_allocation}\")\n</code></pre>"},{"location":"Language-Modeling/Resource%20Accounting/#how-tensors-work-in-pytorch","title":"How Tensors Work in PyTorch","text":"<ul> <li>Tensors are pointers to allocated memory</li> <li>Metadata describes how to access each element</li> </ul>"},{"location":"Language-Modeling/Tokenization/BPE/","title":"Byte Pair Encoding (BPE)","text":"<p>Basic Idea: Train the tokenizer on raw text to automatically determine the vocabulary.</p> <p>Intuition: Common sequences of characters are represented by a single token, while rare sequences are represented by many tokens.</p> <p>Algorithm Sketch: 1. Start with each character as a token (plus an end-of-word marker). 2. Count all adjacent token pairs in the corpus. 3. Merge the most frequent pair into a new token. 4. Repeat for a fixed number of merges.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#how-it-works","title":"How It Works","text":"<p>BPE is an algorithm originally used for data compression. In NLP, it's used to build subword vocabularies:</p> <ol> <li>Initialization: Start with all characters in your corpus as separate tokens.</li> <li>Iterative Merging: Repeatedly merge the most frequent adjacent pair of tokens.</li> <li>Stopping Criterion: Stop after a certain number of merges (vocabulary size).</li> </ol>"},{"location":"Language-Modeling/Tokenization/BPE/#step-by-step-bpe-process","title":"Step-by-Step BPE Process","text":"<p>Let's walk through a simplified example of how BPE works with these words: <pre><code>low lower lowest\nnewer wider\n</code></pre></p>"},{"location":"Language-Modeling/Tokenization/BPE/#initial-vocabulary","title":"Initial Vocabulary","text":"<p>Each word is split into characters with an end-of-word marker: - <code>l, o, w, &lt;/w&gt;</code> (low) - <code>l, o, w, e, r, &lt;/w&gt;</code> (lower) - <code>l, o, w, e, s, t, &lt;/w&gt;</code> (lowest) - <code>n, e, w, e, r, &lt;/w&gt;</code> (newer) - <code>w, i, d, e, r, &lt;/w&gt;</code> (wider)</p>"},{"location":"Language-Modeling/Tokenization/BPE/#first-few-merge-operations","title":"First Few Merge Operations","text":"<ol> <li>Find most frequent pair: <code>e, r</code> (appears 3 times)</li> <li>Merge: <code>e + r \u2192 er</code></li> <li> <p>Vocabulary becomes: <code>l, o, w, &lt;/w&gt;</code>, <code>l, o, w, er, &lt;/w&gt;</code>, <code>l, o, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>l, o</code> (appears 3 times)</p> </li> <li>Merge: <code>l + o \u2192 lo</code></li> <li> <p>Vocabulary becomes: <code>lo, w, &lt;/w&gt;</code>, <code>lo, w, er, &lt;/w&gt;</code>, <code>lo, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>lo, w</code> (appears 3 times)</p> </li> <li>Merge: <code>lo + w \u2192 low</code></li> <li>Vocabulary becomes: <code>low, &lt;/w&gt;</code>, <code>low, er, &lt;/w&gt;</code>, <code>low, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></li> </ol> <p>...and so on.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#visual-progression-for-lowest","title":"Visual Progression for \"lowest\"","text":"<pre><code>Initial:  l   o   w   e   s   t   &lt;/w&gt;\nMerge 1:  l   o   w   e   s   t   &lt;/w&gt;  (er doesn't appear)\nMerge 2:  lo  w   e   s   t   &lt;/w&gt;      (lo created)\nMerge 3:  low e   s   t   &lt;/w&gt;          (low created)\n...\nFinal:    low est &lt;/w&gt;                   (after additional merges)\n</code></pre> <p>This demonstrates how the algorithm gradually builds larger subword units based on frequency.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#implementation","title":"Implementation","text":"<p>The Python implementation includes several key functions that work together to perform Byte Pair Encoding:</p>"},{"location":"Language-Modeling/Tokenization/BPE/#1-statistics-collection-function","title":"1. Statistics Collection Function","text":"<pre><code>def get_stats(vocab):\n    pairs = Counter()\n    for word, freq in vocab.items():\n        symbols = word\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n</code></pre> <p>This function counts the frequency of adjacent token pairs in the vocabulary. For each word in our vocabulary: - It iterates through all adjacent pairs of symbols - It increments a counter for each pair, weighted by how frequently the word appears - The result is a Counter object mapping each pair to its frequency across the corpus</p>"},{"location":"Language-Modeling/Tokenization/BPE/#2-vocabulary-merging-function","title":"2. Vocabulary Merging Function","text":"<pre><code>def merge_vocab(pair, vocab):\n    new_vocab = {}\n    bigram = ''.join(pair)\n    for word in vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_vocab[tuple(new_word)] = vocab[word]\n    return new_vocab\n</code></pre> <p>This function applies a specific merge operation to the entire vocabulary: - It creates a new bigram by joining the pair of tokens - For each word in the vocabulary, it replaces all occurrences of the specific pair with the new merged token - It preserves word frequencies in the new vocabulary - It returns a new vocabulary with the merge applied throughout</p>"},{"location":"Language-Modeling/Tokenization/BPE/#3-bpe-training-function","title":"3. BPE Training Function","text":"<pre><code>def bpe(corpus, num_merges):\n    vocab = Counter()\n    for line in corpus:\n        for word in line.split():\n            vocab[tuple(word) + ('&lt;/w&gt;',)] += 1\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = merge_vocab(best, vocab)\n        merges.append(best)\n    return merges, vocab\n</code></pre> <p>The main BPE algorithm function: - Initializes a vocabulary where each word is split into individual characters plus an end-of-word marker <code>&lt;/w&gt;</code> - Repeatedly:   - Counts pair frequencies with <code>get_stats()</code>   - Finds the most frequent pair   - Applies the merge operation with <code>merge_vocab()</code>   - Records the merge for later use in encoding - Returns both the sequence of merges and the final vocabulary</p>"},{"location":"Language-Modeling/Tokenization/BPE/#4-encoding-function","title":"4. Encoding Function","text":"<pre><code>def encode(word, merges):\n    word = tuple(word) + ('&lt;/w&gt;',)\n    for pair in merges:\n        i = 0\n        new_word = []\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(''.join(pair))\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        word = tuple(new_word)\n    # Handle the end-of-word marker\n    if word[-1] == '&lt;/w&gt;':\n        return word[:-1]\n    else:\n        last = word[-1].replace('&lt;/w&gt;', '')\n        return word[:-1] + (last,) if last else word[:-1]\n</code></pre> <p>This function tokenizes new words using the learned BPE merges: - It starts with the word split into individual characters plus the end marker - It applies each merge operation in the same order they were learned during training - It handles the end-of-word marker properly when returning the final tokenized word</p>"},{"location":"Language-Modeling/Tokenization/BPE/#usage-example","title":"Usage Example","text":"<p>Here's a complete example showing how to use the BPE implementation:</p> <pre><code>from bpe import bpe, encode\n\n# Define a small corpus for training\ncorpus = [\n    \"low lower lowest\",\n    \"newer wider\",\n]\n\n# Train the BPE model with 20 merge operations\nmerges, vocab = bpe(corpus, num_merges=20)\n\n# Print the sequence of merges learned\nprint(\"Learned merges:\")\nfor i, merge in enumerate(merges):\n    print(f\"Merge {i+1}: {merge[0]} + {merge[1]} \u2192 {''.join(merge)}\")\n\n# Print the final vocabulary\nprint(\"\\nFinal vocabulary:\")\nfor word, freq in vocab.items():\n    print(f\"{word}: {freq}\")\n\n# Encode a new word using the learned merges\nencoded = encode(\"lowest\", merges)\nprint(\"\\nEncoded 'lowest':\", encoded)\n</code></pre> <p>This example demonstrates: 1. Training: Creating a BPE model from a small corpus 2. Inspection: Viewing the learned merges and resulting vocabulary 3. Application: Using the model to encode a new word</p> <p>When you run this code, you'll see how the word \"lowest\" gets tokenized according to the subword units learned during training.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#advantages-of-bpe","title":"Advantages of BPE","text":"<ul> <li>Handles out-of-vocabulary words gracefully</li> <li>Balances word-level and character-level representations</li> <li>Efficient for morphologically rich languages</li> <li>Widely used in modern NLP systems (GPT, BERT, etc.)</li> </ul> <p>Basic Idea: Train the tokenizer on raw text to automatically determine the vocabulary.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#implementation-python","title":"Implementation (Python)","text":"<p>The following is a compact reference implementation of BPE. The implementation is included here so you don't need a separate Python file.</p> <pre><code>from collections import Counter\n\ndef get_stats(vocab):\n    \"\"\"Count frequency of all symbol pairs in the vocab.\"\"\"\n    pairs = Counter()\n    for word, freq in vocab.items():\n        symbols = word\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n\ndef merge_vocab(pair, vocab):\n    \"\"\"Merge all occurrences of the most frequent pair in the vocab.\"\"\"\n    new_vocab = {}\n    bigram = ''.join(pair)\n    for word in vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_vocab[tuple(new_word)] = vocab[word]\n    return new_vocab\n\ndef bpe(corpus, num_merges):\n    \"\"\"Train BPE on a corpus and return the merges and final vocabulary.\"\"\"\n    vocab = Counter()\n    for line in corpus:\n        for word in line.split():\n            vocab[tuple(word) + ('&lt;/w&gt;',)] += 1\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = merge_vocab(best, vocab)\n        merges.append(best)\n    return merges, vocab\n\ndef encode(word, merges):\n    \"\"\"Encode a word using learned BPE merges.\"\"\"\n    word = tuple(word) + ('&lt;/w&gt;',)\n    for pair in merges:\n        i = 0\n        new_word = []\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(''.join(pair))\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        word = tuple(new_word)\n    if word[-1] == '&lt;/w&gt;':\n        return word[:-1]\n    else:\n        last = word[-1].replace('&lt;/w&gt;', '')\n        return word[:-1] + (last,) if last else word[:-1]\n\n# Example\nif __name__ == \"__main__\":\n    corpus = [\n        \"low lower lowest\",\n        \"newer wider\",\n    ]\n    merges, vocab = bpe(corpus, num_merges=20)\n    print(\"Learned merges:\", merges)\n    print(\"Final vocab sample:\", list(vocab.items())[:10])\n    print(\"Encoded 'lowest':\", encode(\"lowest\", merges))\n    print(\"Encoded 'lowest':\", encode(\"lowest\", merges))\n</code></pre> <p>Created by Saqlain.</p>"}]}