{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NLP-101 Documentation","text":"<ul> <li>Natural Language Processing concepts and techniques</li> <li>Key modules:<ul> <li>Language Modeling</li> </ul> </li> </ul>"},{"location":"Language-Modeling/","title":"Language Modeling","text":"<p>Welcome to the Language Modeling module of NLP-101. This section covers the core concepts, techniques, and practical implementations in language modeling.</p>"},{"location":"Language-Modeling/#module-topics-resources","title":"Module Topics &amp; Resources","text":"Topic Description Resources Byte Pair Encoding (BPE) Subword tokenization algorithm for representing common character sequences BPE Guide (implementation included in this README) Resource Accounting Tensor memory, precision types, and training efficiency in deep learning Resource Accounting Guide Architectures &amp; Hyperparameters Guidance on model architecture choices, normalization, activations, and hyperparameter trade-offs Architectures &amp; Hyperparameters"},{"location":"Language-Modeling/#what-is-language-modeling","title":"What is Language Modeling?","text":"<p>Language modeling is a fundamental task in Natural Language Processing (NLP) that involves predicting the probability of a sequence of words. It is the backbone of many modern NLP applications, including machine translation and text generation.</p>"},{"location":"Language-Modeling/#learning-objectives","title":"Learning Objectives","text":"Objective Description Basic Concepts Understand fundamental concepts of language modeling and probability distributions over text Tokenization Techniques Learn different tokenization approaches including character, word, and subword tokenization Implementation Implement and experiment with various language modeling techniques"},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/","title":"Normalization and Activations","text":""},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/#normalization","title":"Normalization","text":""},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/#layer-normalization-layernorm","title":"Layer Normalization (LayerNorm)","text":"<ul> <li>Used in the original Transformer (\"Attention Is All You Need\").</li> <li>Formula: let x be a vector, \u03bc = mean(x), \u03c3^2 = var(x):</li> </ul> <p>The LayerNorm operation is commonly written as:</p> <p>$$   \\mathrm{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta   $$</p> <ul> <li>Pre-LN vs Post-LN:</li> <li>Post-LN (original): normalization after the residual block.</li> <li>Pre-LN (common now): normalization before the sub-layer (attention/FFN). Pre-LN often improves gradient stability and training dynamics for deep models.</li> </ul>"},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/#rms-normalization-rmsnorm","title":"RMS Normalization (RMSNorm)","text":"<ul> <li>RMSNorm removes the mean-centering step and uses root-mean-square for normalization. It has fewer operations and parameters than LayerNorm and can be faster/more memory efficient.</li> </ul> <p>A common RMSNorm formulation is:</p> <p>$$   \\mathrm{RMSNorm}(x) = \\gamma \\frac{x}{\\sqrt{\\tfrac{1}{d}\\sum_{i=1}^d x_i^2 + \\varepsilon}}   $$</p> <ul> <li>Notes:</li> <li>No trainable bias (\u03b2) in the basic RMSNorm formulation.</li> <li>Slightly cheaper (no subtraction/mean computation) and used in many modern architectures.</li> </ul>"},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/#why-choose-one-vs-the-other","title":"Why choose one vs the other","text":"<ul> <li>LayerNorm provides full centering and scaling; RMSNorm is a lightweight alternative with similar empirical performance in many cases.</li> <li>Pre-LN residual blocks + either norm usually give stable training for deep transformers.</li> </ul>"},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/#feed-forward-layers-ffn-common-forms","title":"Feed-Forward Layers (FFN) \u2014 common forms","text":"<ul> <li>Standard: FFN(x) = W2 * f(W1 x + b1) + b2</li> <li>Standard: </li> </ul> <p>$$   \\mathrm{FFN}(x) = W_2\\, f(W_1 x + b_1) + b_2   $$</p> <ul> <li>W\u2081: input \u2192 hidden, activation f (ReLU/GeLU/etc.), W\u2082: hidden \u2192 output.</li> <li>Gated/Mixture variants are common (GLU/GeGLU/SwiGLU) where activations include element-wise gating for improved expressivity.</li> </ul>"},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/#activations","title":"Activations","text":"<p>Common activation functions used in transformer FFNs and variants:</p> <ul> <li>ReLU: f(x) = max(0, x)</li> <li>ReLU: </li> </ul> <p>$$   \\mathrm{ReLU}(x) = \\max(0, x)   $$   - Simple, cheap; used in early Transformers.</p> <ul> <li>GeLU (Gaussian Error Linear Unit):</li> <li>GeLU (Gaussian Error Linear Unit):</li> <li>Common approximation:</li> </ul> <p>$$   \\mathrm{GeLU}(x) \\approx x\\,\\Phi(x)   $$</p> <p>where \u03a6 is the standard normal CDF. Many implementations use a fast approximation for efficiency.   - Used in GPT and many transformer variants; smoother than ReLU.</p> <ul> <li>SiLU / Swish: f(x) = x * sigmoid(x)</li> <li>SiLU / Swish:</li> </ul> <p>$$   \\mathrm{SiLU}(x) = x\\,\\sigma(x)   $$</p> <p>where \u03c3(x)=1/(1+e^{-x}) is the logistic sigmoid.   - Smooth, can improve performance in some models.</p> <ul> <li>Gated activations:   FF ReGLU:</li> </ul> <p>$$   \\mathrm{FF ReGLU}(x) = (\\max(0, xW_1) \\otimes (xV)) W_2   $$</p> <p>FF GeGLU:</p> <p>$$   \\mathrm{FF GeGLU}(x, W, V, W_2) = (\\mathrm{GLU}(xW) \\otimes (xV)) W_2   $$</p> <p>SwiGLU (Swish is x\\cdot\\sigma(x)):</p> <p>$$   \\mathrm{FF SwiGLU}(x, W, V, W_2) = (\\mathrm{Swish}(xW) \\otimes (xV)) W_2   $$</p> <p>V -&gt; extra parameter   Note: Gated models use smaller dimensions for dff by \u2154</p>"},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/#practical-notes","title":"Practical notes","text":"<ul> <li>Most modern transformer implementations use GeLU (or a gated variant) in the FFN.</li> <li>When optimizing for memory/speed, consider RMSNorm + GeLU (or gated GeLU) with pre-LN transformer blocks.</li> </ul>"},{"location":"Language-Modeling/Architectures%20and%20Hyperparameters/#serial-vs-parallel-layers","title":"Serial vs Parallel Layers","text":"<p>In standard transformer blocks, layers are computed serially: first attention, then MLP.</p> <p>Some recent models use a parallel formulation, where the MLP and attention operate in parallel on the same normalized input.</p> <p>Serial (standard) formulation:</p> <p> $$ y = x + \\mathrm{MLP}(\\mathrm{LayerNorm}(x + \\mathrm{Attention}(\\mathrm{LayerNorm}(x)))) $$ </p> <p>Parallel formulation:</p> <p> $$ y = x + \\mathrm{MLP}(\\mathrm{LayerNorm}(x)) + \\mathrm{Attention}(\\mathrm{LayerNorm}(x)) $$ </p> <p>Benefits: - Parallel layers enable the MLP and attention input matrix multiplications to be fused, resulting in ~15% faster training at large scale. - Ablation experiments show a small quality drop at 8B scale, but no degradation at 62B scale; extrapolation suggests parallel layers are quality-neutral at even larger scales (e.g., 540B).</p>"},{"location":"Language-Modeling/Resource%20Accounting/","title":"Resource Accounting","text":""},{"location":"Language-Modeling/Resource%20Accounting/#key-points","title":"Key Points","text":"<ul> <li>Tensors are the core data structure in machine learning, used for:</li> <li>Parameters</li> <li>Gradients</li> <li>Optimizer states</li> <li>Data activations</li> </ul>"},{"location":"Language-Modeling/Resource%20Accounting/#tensor-memory-types","title":"Tensor Memory Types","text":"<ul> <li>float32 (single precision):</li> <li>Standard for full precision in ML</li> <li> <p>Good balance of range and accuracy</p> </li> <li> <p>float16 (half precision):</p> </li> <li>Uses less memory</li> <li> <p>Not ideal for very small numbers</p> </li> <li> <p>bfloat16 (brain floating point):</p> </li> <li>Same memory as float32</li> <li> <p>More bits for exponent, fewer for fraction</p> </li> <li> <p>fp8 (8-bit floating point):</p> </li> <li>Very low memory usage</li> </ul> <p>Memory usage depends on: - Number of values - Data type of each value</p>"},{"location":"Language-Modeling/Resource%20Accounting/#training-implications","title":"Training Implications","text":"<ul> <li>Training with float32 is stable but memory-intensive</li> <li>Training with float16, bfloat16, or fp8 saves memory but can cause instability</li> <li>Mixed precision training is common (e.g., float32 for attention, bfloat16 for feed-forward)</li> <li>Tensors are on CPU by default; use GPU for acceleration</li> </ul>"},{"location":"Language-Modeling/Resource%20Accounting/#pytorch-example-device-memory","title":"PyTorch Example: Device &amp; Memory","text":"<pre><code>import torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")\nnum_gpus = torch.cuda.device_count()\nfor i in range(num_gpus):\n    properties = torch.cuda.get_device_properties(i)\n    print(properties)\nmemory_allocation = torch.cuda.memory_allocated() if device == 'cuda' else None\nprint(f\"Memory allocated: {memory_allocation}\")\n</code></pre>"},{"location":"Language-Modeling/Resource%20Accounting/#how-tensors-work-in-pytorch","title":"How Tensors Work in PyTorch","text":"<ul> <li>Tensors are pointers to allocated memory</li> <li>Metadata describes how to access each element</li> </ul>"},{"location":"Language-Modeling/Tokenization/BPE/","title":"Byte Pair Encoding (BPE)","text":"<p>Basic Idea: Train the tokenizer on raw text to automatically determine the vocabulary.</p> <p>Intuition: Common sequences of characters are represented by a single token, while rare sequences are represented by many tokens.</p> <p>Algorithm Sketch: 1. Start with each character as a token (plus an end-of-word marker). 2. Count all adjacent token pairs in the corpus. 3. Merge the most frequent pair into a new token. 4. Repeat for a fixed number of merges.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#how-it-works","title":"How It Works","text":"<p>BPE is an algorithm originally used for data compression. In NLP, it's used to build subword vocabularies:</p> <ol> <li>Initialization: Start with all characters in your corpus as separate tokens.</li> <li>Iterative Merging: Repeatedly merge the most frequent adjacent pair of tokens.</li> <li>Stopping Criterion: Stop after a certain number of merges (vocabulary size).</li> </ol>"},{"location":"Language-Modeling/Tokenization/BPE/#step-by-step-bpe-process","title":"Step-by-Step BPE Process","text":"<p>Let's walk through a simplified example of how BPE works with these words: <pre><code>low lower lowest\nnewer wider\n</code></pre></p>"},{"location":"Language-Modeling/Tokenization/BPE/#initial-vocabulary","title":"Initial Vocabulary","text":"<p>Each word is split into characters with an end-of-word marker: - <code>l, o, w, &lt;/w&gt;</code> (low) - <code>l, o, w, e, r, &lt;/w&gt;</code> (lower) - <code>l, o, w, e, s, t, &lt;/w&gt;</code> (lowest) - <code>n, e, w, e, r, &lt;/w&gt;</code> (newer) - <code>w, i, d, e, r, &lt;/w&gt;</code> (wider)</p>"},{"location":"Language-Modeling/Tokenization/BPE/#first-few-merge-operations","title":"First Few Merge Operations","text":"<ol> <li>Find most frequent pair: <code>e, r</code> (appears 3 times)</li> <li>Merge: <code>e + r \u2192 er</code></li> <li> <p>Vocabulary becomes: <code>l, o, w, &lt;/w&gt;</code>, <code>l, o, w, er, &lt;/w&gt;</code>, <code>l, o, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>l, o</code> (appears 3 times)</p> </li> <li>Merge: <code>l + o \u2192 lo</code></li> <li> <p>Vocabulary becomes: <code>lo, w, &lt;/w&gt;</code>, <code>lo, w, er, &lt;/w&gt;</code>, <code>lo, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>lo, w</code> (appears 3 times)</p> </li> <li>Merge: <code>lo + w \u2192 low</code></li> <li>Vocabulary becomes: <code>low, &lt;/w&gt;</code>, <code>low, er, &lt;/w&gt;</code>, <code>low, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></li> </ol> <p>...and so on.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#visual-progression-for-lowest","title":"Visual Progression for \"lowest\"","text":"<pre><code>Initial:  l   o   w   e   s   t   &lt;/w&gt;\nMerge 1:  l   o   w   e   s   t   &lt;/w&gt;  (er doesn't appear)\nMerge 2:  lo  w   e   s   t   &lt;/w&gt;      (lo created)\nMerge 3:  low e   s   t   &lt;/w&gt;          (low created)\n...\nFinal:    low est &lt;/w&gt;                   (after additional merges)\n</code></pre> <p>This demonstrates how the algorithm gradually builds larger subword units based on frequency.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#implementation","title":"Implementation","text":"<p>The Python implementation includes several key functions that work together to perform Byte Pair Encoding:</p>"},{"location":"Language-Modeling/Tokenization/BPE/#1-statistics-collection-function","title":"1. Statistics Collection Function","text":"<pre><code>def get_stats(vocab):\n    pairs = Counter()\n    for word, freq in vocab.items():\n        symbols = word\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n</code></pre> <p>This function counts the frequency of adjacent token pairs in the vocabulary. For each word in our vocabulary: - It iterates through all adjacent pairs of symbols - It increments a counter for each pair, weighted by how frequently the word appears - The result is a Counter object mapping each pair to its frequency across the corpus</p>"},{"location":"Language-Modeling/Tokenization/BPE/#2-vocabulary-merging-function","title":"2. Vocabulary Merging Function","text":"<pre><code>def merge_vocab(pair, vocab):\n    new_vocab = {}\n    bigram = ''.join(pair)\n    for word in vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_vocab[tuple(new_word)] = vocab[word]\n    return new_vocab\n</code></pre> <p>This function applies a specific merge operation to the entire vocabulary: - It creates a new bigram by joining the pair of tokens - For each word in the vocabulary, it replaces all occurrences of the specific pair with the new merged token - It preserves word frequencies in the new vocabulary - It returns a new vocabulary with the merge applied throughout</p>"},{"location":"Language-Modeling/Tokenization/BPE/#3-bpe-training-function","title":"3. BPE Training Function","text":"<pre><code>def bpe(corpus, num_merges):\n    vocab = Counter()\n    for line in corpus:\n        for word in line.split():\n            vocab[tuple(word) + ('&lt;/w&gt;',)] += 1\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = merge_vocab(best, vocab)\n        merges.append(best)\n    return merges, vocab\n</code></pre> <p>The main BPE algorithm function: - Initializes a vocabulary where each word is split into individual characters plus an end-of-word marker <code>&lt;/w&gt;</code> - Repeatedly:   - Counts pair frequencies with <code>get_stats()</code>   - Finds the most frequent pair   - Applies the merge operation with <code>merge_vocab()</code>   - Records the merge for later use in encoding - Returns both the sequence of merges and the final vocabulary</p>"},{"location":"Language-Modeling/Tokenization/BPE/#4-encoding-function","title":"4. Encoding Function","text":"<pre><code>def encode(word, merges):\n    word = tuple(word) + ('&lt;/w&gt;',)\n    for pair in merges:\n        i = 0\n        new_word = []\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(''.join(pair))\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        word = tuple(new_word)\n    # Handle the end-of-word marker\n    if word[-1] == '&lt;/w&gt;':\n        return word[:-1]\n    else:\n        last = word[-1].replace('&lt;/w&gt;', '')\n        return word[:-1] + (last,) if last else word[:-1]\n</code></pre> <p>This function tokenizes new words using the learned BPE merges: - It starts with the word split into individual characters plus the end marker - It applies each merge operation in the same order they were learned during training - It handles the end-of-word marker properly when returning the final tokenized word</p>"},{"location":"Language-Modeling/Tokenization/BPE/#usage-example","title":"Usage Example","text":"<p>Here's a complete example showing how to use the BPE implementation:</p> <pre><code>from bpe import bpe, encode\n\n# Define a small corpus for training\ncorpus = [\n    \"low lower lowest\",\n    \"newer wider\",\n]\n\n# Train the BPE model with 20 merge operations\nmerges, vocab = bpe(corpus, num_merges=20)\n\n# Print the sequence of merges learned\nprint(\"Learned merges:\")\nfor i, merge in enumerate(merges):\n    print(f\"Merge {i+1}: {merge[0]} + {merge[1]} \u2192 {''.join(merge)}\")\n\n# Print the final vocabulary\nprint(\"\\nFinal vocabulary:\")\nfor word, freq in vocab.items():\n    print(f\"{word}: {freq}\")\n\n# Encode a new word using the learned merges\nencoded = encode(\"lowest\", merges)\nprint(\"\\nEncoded 'lowest':\", encoded)\n</code></pre> <p>This example demonstrates: 1. Training: Creating a BPE model from a small corpus 2. Inspection: Viewing the learned merges and resulting vocabulary 3. Application: Using the model to encode a new word</p> <p>When you run this code, you'll see how the word \"lowest\" gets tokenized according to the subword units learned during training.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#advantages-of-bpe","title":"Advantages of BPE","text":"<ul> <li>Handles out-of-vocabulary words gracefully</li> <li>Balances word-level and character-level representations</li> <li>Efficient for morphologically rich languages</li> <li>Widely used in modern NLP systems (GPT, BERT, etc.)</li> </ul> <p>Basic Idea: Train the tokenizer on raw text to automatically determine the vocabulary.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#implementation-python","title":"Implementation (Python)","text":"<p>The following is a compact reference implementation of BPE. The implementation is included here so you don't need a separate Python file.</p> <pre><code>from collections import Counter\n\ndef get_stats(vocab):\n    \"\"\"Count frequency of all symbol pairs in the vocab.\"\"\"\n    pairs = Counter()\n    for word, freq in vocab.items():\n        symbols = word\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n\ndef merge_vocab(pair, vocab):\n    \"\"\"Merge all occurrences of the most frequent pair in the vocab.\"\"\"\n    new_vocab = {}\n    bigram = ''.join(pair)\n    for word in vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_vocab[tuple(new_word)] = vocab[word]\n    return new_vocab\n\ndef bpe(corpus, num_merges):\n    \"\"\"Train BPE on a corpus and return the merges and final vocabulary.\"\"\"\n    vocab = Counter()\n    for line in corpus:\n        for word in line.split():\n            vocab[tuple(word) + ('&lt;/w&gt;',)] += 1\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = merge_vocab(best, vocab)\n        merges.append(best)\n    return merges, vocab\n\ndef encode(word, merges):\n    \"\"\"Encode a word using learned BPE merges.\"\"\"\n    word = tuple(word) + ('&lt;/w&gt;',)\n    for pair in merges:\n        i = 0\n        new_word = []\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(''.join(pair))\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        word = tuple(new_word)\n    if word[-1] == '&lt;/w&gt;':\n        return word[:-1]\n    else:\n        last = word[-1].replace('&lt;/w&gt;', '')\n        return word[:-1] + (last,) if last else word[:-1]\n\n# Example\nif __name__ == \"__main__\":\n    corpus = [\n        \"low lower lowest\",\n        \"newer wider\",\n    ]\n    merges, vocab = bpe(corpus, num_merges=20)\n    print(\"Learned merges:\", merges)\n    print(\"Final vocab sample:\", list(vocab.items())[:10])\n    print(\"Encoded 'lowest':\", encode(\"lowest\", merges))\n    print(\"Encoded 'lowest':\", encode(\"lowest\", merges))\n</code></pre> <p>Created by Saqlain.</p>"}]}