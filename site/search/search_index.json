{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NLP-101","text":"<p>Welcome to NLP-101 - a comprehensive introduction to Natural Language Processing (NLP), covering fundamental concepts and advanced techniques that power modern NLP systems.</p>"},{"location":"#what-is-nlp","title":"What is NLP?","text":"<p>Natural Language Processing is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to read, understand, and derive meaning from human languages in a valuable way.</p>"},{"location":"#course-structure","title":"Course Structure","text":"<p>This course is organized into modules, each covering essential aspects of NLP:</p>"},{"location":"#1-language-modeling","title":"1. Language Modeling","text":"<p>Learn about the fundamentals of language modeling, including tokenization techniques and how to build language models from scratch.</p>"},{"location":"#about-this-course","title":"About This Course","text":"<p>NLP-101 provides: - Clear, well-documented implementations of core NLP concepts - Detailed explanations of algorithms and techniques - Hands-on examples you can experiment with - Progressive learning from basics to advanced topics</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Explore the course modules in the navigation menu, or clone the repository to experiment with the code:</p> <pre><code>git clone https://github.com/saqlain2204/Language-Modeling-101.git\ncd Language-Modeling-101\n</code></pre> <p>Created by Saqlain</p>"},{"location":"Language-Modeling/","title":"Language Modeling","text":"<p>Welcome to the Language Modeling module of NLP-101. This section covers fundamental concepts and implementations in language modeling.</p>"},{"location":"Language-Modeling/#topics-covered","title":"Topics Covered","text":""},{"location":"Language-Modeling/#1-tokenization","title":"1. Tokenization","text":"<ul> <li>Byte Pair Encoding (BPE)</li> <li>Understanding BPE algorithm</li> <li>Implementation in Python</li> <li>Examples and use cases</li> </ul>"},{"location":"Language-Modeling/#what-is-language-modeling","title":"What is Language Modeling?","text":"<p>Language modeling is a fundamental task in Natural Language Processing that involves predicting the probability of a sequence of words. It forms the backbone of many modern NLP applications, from machine translation to text generation.</p>"},{"location":"Language-Modeling/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand basic concepts of language modeling</li> <li>Learn different tokenization techniques</li> <li>Implement and experiment with various language modeling approaches</li> </ul>"},{"location":"Language-Modeling/Tokenization/BPE/","title":"Byte Pair Encoding (BPE)","text":"<p>Basic Idea: Train the tokenizer on raw text to automatically determine the vocabulary.</p> <p>Intuition: Common sequences of characters are represented by a single token, rare sequences are represented by many tokens.</p> <p>Algorithm Sketch: 1. Start with each character as a token (plus an end-of-word marker). 2. Count all adjacent token pairs in the corpus. 3. Merge the most frequent pair into a new token. 4. Repeat for a fixed number of merges.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#how-it-works","title":"How It Works","text":"<p>BPE is an algorithm originally used for data compression. In NLP, it's used to build subword vocabularies:</p> <ol> <li>Initialization: Start with all characters in your corpus as separate tokens</li> <li>Iterative Merging: Repeatedly merge the most frequent adjacent pair of tokens</li> <li>Stopping Criterion: Stop after a certain number of merges (vocabulary size)</li> </ol>"},{"location":"Language-Modeling/Tokenization/BPE/#step-by-step-bpe-process","title":"Step-by-Step BPE Process","text":"<p>Let's walk through a simplified example of how BPE works with these words: <pre><code>low lower lowest\nnewer wider\n</code></pre></p>"},{"location":"Language-Modeling/Tokenization/BPE/#initial-vocabulary","title":"Initial Vocabulary","text":"<p>Each word is split into characters with an end-of-word marker: - <code>l, o, w, &lt;/w&gt;</code> (low) - <code>l, o, w, e, r, &lt;/w&gt;</code> (lower) - <code>l, o, w, e, s, t, &lt;/w&gt;</code> (lowest) - <code>n, e, w, e, r, &lt;/w&gt;</code> (newer) - <code>w, i, d, e, r, &lt;/w&gt;</code> (wider)</p>"},{"location":"Language-Modeling/Tokenization/BPE/#first-few-merge-operations","title":"First Few Merge Operations","text":"<ol> <li>Find most frequent pair: <code>e, r</code> (appears 3 times)</li> <li>Merge: <code>e + r \u2192 er</code></li> <li> <p>Vocabulary becomes: <code>l, o, w, &lt;/w&gt;</code>, <code>l, o, w, er, &lt;/w&gt;</code>, <code>l, o, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>l, o</code> (appears 3 times)</p> </li> <li>Merge: <code>l + o \u2192 lo</code></li> <li> <p>Vocabulary becomes: <code>lo, w, &lt;/w&gt;</code>, <code>lo, w, er, &lt;/w&gt;</code>, <code>lo, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>lo, w</code> (appears 3 times)</p> </li> <li>Merge: <code>lo + w \u2192 low</code></li> <li>Vocabulary becomes: <code>low, &lt;/w&gt;</code>, <code>low, er, &lt;/w&gt;</code>, <code>low, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></li> </ol> <p>...and so on.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#visual-progression-for-lowest","title":"Visual Progression for \"lowest\"","text":"<pre><code>Initial:  l   o   w   e   s   t   &lt;/w&gt;\nMerge 1:  l   o   w   e   s   t   &lt;/w&gt;  (er doesn't appear)\nMerge 2:  lo  w   e   s   t   &lt;/w&gt;      (lo created)\nMerge 3:  low e   s   t   &lt;/w&gt;          (low created)\n...\nFinal:    low est &lt;/w&gt;                   (after additional merges)\n</code></pre> <p>This demonstrates how the algorithm gradually builds larger subword units based on frequency.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#implementation","title":"Implementation","text":"<p>The Python implementation includes several key functions that work together to perform Byte Pair Encoding:</p>"},{"location":"Language-Modeling/Tokenization/BPE/#1-statistics-collection-function","title":"1. Statistics Collection Function","text":"<pre><code>def get_stats(vocab):\n    pairs = Counter()\n    for word, freq in vocab.items():\n        symbols = word\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n</code></pre> <p>This function counts the frequency of adjacent token pairs in the vocabulary. For each word in our vocabulary: - It iterates through all adjacent pairs of symbols - It increments a counter for each pair, weighted by how frequently the word appears - The result is a Counter object mapping each pair to its frequency across the corpus</p>"},{"location":"Language-Modeling/Tokenization/BPE/#2-vocabulary-merging-function","title":"2. Vocabulary Merging Function","text":"<pre><code>def merge_vocab(pair, vocab):\n    new_vocab = {}\n    bigram = ''.join(pair)\n    for word in vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_vocab[tuple(new_word)] = vocab[word]\n    return new_vocab\n</code></pre> <p>This function applies a specific merge operation to the entire vocabulary: - It creates a new bigram by joining the pair of tokens - For each word in the vocabulary, it replaces all occurrences of the specific pair with the new merged token - It preserves word frequencies in the new vocabulary - It returns a new vocabulary with the merge applied throughout</p>"},{"location":"Language-Modeling/Tokenization/BPE/#3-bpe-training-function","title":"3. BPE Training Function","text":"<pre><code>def bpe(corpus, num_merges):\n    vocab = Counter()\n    for line in corpus:\n        for word in line.split():\n            vocab[tuple(word) + ('&lt;/w&gt;',)] += 1\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = merge_vocab(best, vocab)\n        merges.append(best)\n    return merges, vocab\n</code></pre> <p>The main BPE algorithm function that: - Initializes a vocabulary where each word is split into individual characters plus an end-of-word marker <code>&lt;/w&gt;</code> - Repeatedly:   - Counts pair frequencies with <code>get_stats()</code>   - Finds the most frequent pair   - Applies the merge operation with <code>merge_vocab()</code>   - Records the merge for later use in encoding - Returns both the sequence of merges and the final vocabulary</p>"},{"location":"Language-Modeling/Tokenization/BPE/#4-encoding-function","title":"4. Encoding Function","text":"<pre><code>def encode(word, merges):\n    word = tuple(word) + ('&lt;/w&gt;',)\n    for pair in merges:\n        i = 0\n        new_word = []\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(''.join(pair))\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        word = tuple(new_word)\n    # Handle the end-of-word marker\n    if word[-1] == '&lt;/w&gt;':\n        return word[:-1]\n    else:\n        last = word[-1].replace('&lt;/w&gt;', '')\n        return word[:-1] + (last,) if last else word[:-1]\n</code></pre> <p>This function tokenizes new words using the learned BPE merges: - It starts with the word split into individual characters plus the end marker - It applies each merge operation in the same order they were learned during training - It handles the end-of-word marker properly when returning the final tokenized word</p>"},{"location":"Language-Modeling/Tokenization/BPE/#usage-example","title":"Usage Example","text":"<p>Here's a complete example showing how to use the BPE implementation:</p> <pre><code>from bpe import bpe, encode\n\n# Define a small corpus for training\ncorpus = [\n    \"low lower lowest\",\n    \"newer wider\",\n]\n\n# Train the BPE model with 20 merge operations\nmerges, vocab = bpe(corpus, num_merges=20)\n\n# Print the sequence of merges learned\nprint(\"Learned merges:\")\nfor i, merge in enumerate(merges):\n    print(f\"Merge {i+1}: {merge[0]} + {merge[1]} \u2192 {''.join(merge)}\")\n\n# Print the final vocabulary\nprint(\"\\nFinal vocabulary:\")\nfor word, freq in vocab.items():\n    print(f\"{word}: {freq}\")\n\n# Encode a new word using the learned merges\nencoded = encode(\"lowest\", merges)\nprint(\"\\nEncoded 'lowest':\", encoded)\n</code></pre> <p>This example demonstrates: 1. Training: Creating a BPE model from a small corpus 2. Inspection: Viewing the learned merges and resulting vocabulary 3. Application: Using the model to encode a new word</p> <p>When you run this code, you'll see how the word \"lowest\" gets tokenized according to the subword units learned during training.</p>"},{"location":"Language-Modeling/Tokenization/BPE/#advantages-of-bpe","title":"Advantages of BPE","text":"<ul> <li>Handles out-of-vocabulary words gracefully</li> <li>Balances word-level and character-level representations</li> <li>Efficient for morphologically rich languages</li> <li>Widely used in modern NLP systems (GPT, BERT, etc.)</li> </ul> <p>Created by Saqlain.</p>"}]}